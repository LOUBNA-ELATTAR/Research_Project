# Fine-Tuning Large Language Models for Domain-Specific Tasks: Application to Educational Assistants

This repository contains the code implementation of the project focused on fine-tuning large language models (LLMs) for domain-specific tasks, particularly in the educational domain. The aim of this project is to investigate the efficacy of fine-tuning techniques for adapting large language models to educational tasks. It addresses research questions such as the effectiveness of fine-tuning at low computational cost and the adaptation of fine-tuning techniques for educational datasets.

## Folders
1. **Datasets:** 
2. **Prompt Engineering:** Includes a notebook explaining prompt engineering techniques in the context of education.
3. **RAG:** Implements RAG (Retrieval Augmented Generation) using Wikipedia dumps.
4. **Finetuning:** Contains folders corresponding to research questions RQ1 and RQ2:
   - **RQ1:** Investigates the effectiveness of fine-tuning LLMs at low computational cost.
   - **RQ2:** Explores fine-tuning techniques tailored for educational domain dataset.
5. **Evaluation:** Includes a notebook implementing a hybrid evaluation protocol combining human evaluations and LLMs as judges

