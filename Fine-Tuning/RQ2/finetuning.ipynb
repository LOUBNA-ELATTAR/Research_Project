{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_32J2Vo-UJg",
        "outputId": "01ebfb2b-2157-4eef-fdca-eac97d377ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPtG4kGR-U5Z",
        "outputId": "c123e16e-0211-43df-9c8d-d68cb015c2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib_We3NLtj2E"
      },
      "outputs": [],
      "source": [
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "dataset_name = \"loubnaelattar/dataset\"\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "lora_r = 64 # LoRA attention dimension\n",
        "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
        "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
        "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
        "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 1 # Number of training epochs\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 0\n",
        "logging_steps = 25\n",
        "max_seq_length = None\n",
        "packing = False\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0c03444db784eefa3a77e9bf8c106c3",
            "ad375b55a41140efaa7857d1fbf90449",
            "780ce6c791c840378350d5b1bbd9081a",
            "0080c90ce2694f6ca9c3587caf3c0873",
            "70721aa42bb04051ab9366ad02059d02",
            "f7e93923f5674782a364f0722c1b96b3",
            "b11c0dae0e754b35b20a4384d8c2e1ca",
            "1c8c078aa8604c6dbbfddb3017e3c3ce",
            "72ef33a06a604741995e45eacf0c75b6",
            "b18942d5e5514f6d8c36f97884e2c7cc",
            "0d6b709469c84ed9b03ff6ad7c2c15d0",
            "5dc2d9860e3e4a75a26145c57f57c840",
            "7b59b4abedc440ee8b71b861cf28f5f4",
            "c43e431bd8ce4a2996e4c721637b5b97",
            "5432817bade34b3cbcbfc5f7d4567c46",
            "7d4d900d96444a2883f446b325e70410",
            "632a633e1c124c29bd5a396f3906d3f6",
            "c7ea990f74b04c07b68ef38f5f6ccd64",
            "ae235bb607c9491480d62f70d67702d2",
            "0693fc110c034aeeb6bb2f4eddf14ac5",
            "ebc5843b52aa41ff9d10f1cebc318a7c",
            "b229966402924aa6baacb7499211fcb8",
            "eef856b26ace4e9b9ae002dab25821dd",
            "e6c4ba3271b84c6ba744c8274acc9268",
            "aa051c784a0642b9b0f838e5c97b2e83",
            "365edd9278044c23903697c934d506ba",
            "d25227e267ad4344b0e1c784e0ef8c1d",
            "e87bab4edf324c458d2c8837e3db3ed9",
            "15ce42f6345c4788a3df4f9ba4c9b098",
            "977b42dc4f7b4cfe866e6e397c426901",
            "64f2c9076ed442aebe4bc4ed289e4813",
            "e69a4c02dc6d494dbb2dc111d97ccebe",
            "55b0152f23eb4b639a22839772f84c03",
            "3bde925d7fe94489acf0a892dd4ea6b2",
            "7b8075701f044cf4bcc5c808552bef8f",
            "7a7026ef074541dab30a73b7b1c344ed",
            "2ec5f6916f9d4a5992890d9fe6a56a6c",
            "238ca3d00f2a4641860da3d990b542df",
            "ce230885ae6c4de78e11af1d1b161974",
            "0da4d1b0e364448eadbdd79b1b95518f",
            "8d7c725eb4414e35b5c56ef6bb4b2748",
            "d9ec12f76d9f423ebbed9c2d2eef7caa",
            "260ec0bd76e34844a4b383c7a4a1d2d5",
            "89e9056928f04cf28e865288ac05950d",
            "b412c6caea304d1d85eec27607a0603e",
            "99cf4161ba114355803651a4c75c51f4",
            "161b44c32b7044698dc6e157832b1fdc",
            "7607c29ba5b44fecaf57ea16125dc8b0",
            "7ddc02d2dbff42a49a0776fedc867b24",
            "01da208c8a494c9fb1697b66fc863316",
            "89a7fe3d815c4b41bb08cc20007981e4",
            "e15f23ffc2b14670a03c0eda89228675",
            "b5434d581a7b41db9d000d82173b0974",
            "8b2d5ed9875a4306b9314fab18258799",
            "98d111b82c834b37b20dc2c47b521024",
            "b6a40906ee274c26925886bbd1571b74",
            "f1e68f7866924424b038190a2779c09c",
            "b448fe19f32649e1ae9ebc9696f489ed",
            "a6e2beb8a9d2457388b42ec5086f3c0d",
            "64b890999afe47dc83d9a379ecf1130a",
            "e68218b427a341c7a9590d13b5807aa9",
            "04b4799438684bab8d884fc42f16bc75",
            "eafac7c52ec14a91b210b23de2bc4c9d",
            "2f050286d10f4c98b10d08ecb1ea5607",
            "c692711ceede4975be960051cdabe9c8",
            "1de1e33ec76f44e2b37a983ad31d069a",
            "a46fb8506c834cd88e0e1a8285a5a37e",
            "a3a334a407a04aef8a97a8d69d8e89b2",
            "694e806cec3d49b2880ae73e70d54135",
            "9d3fc611e0b840868ea55ff673dedb87",
            "5ed169dff0d6479c852eacaa552da195",
            "05320e0d0586472982be033aee1ad4c6",
            "50f2a97b81bf4174bda8a6d7289a1de0",
            "88d8af2e31384bf7825838b16006f49e",
            "ead16ecbe4bd46909f169d99a6ae7c04",
            "dec105d1404b4329a9171a081c5b988f",
            "8fd688a5f22b470198ceeb63b0a22d36",
            "d70d4c81c2db4e5897868a39127daa09",
            "817b674f381e4e34a9ec0e7590d650e6",
            "2b93de4d12c44def9c4ef9766cd18074",
            "8f130267adda47beb0c39812cbf5e7b9",
            "e98455f3d1ea4655931c6c1232fe8341",
            "bdbf4b35ec0f44ab82eb663e83edbc99",
            "3291d939562440c5ba6b7cd4b4bcb9bd",
            "42211cc6c77a4d92abba368420cbe311",
            "a943d5ea6daf4383a88bd0f2aac8973f",
            "6742d301812d444ea83f4a8b639c76d8",
            "518e6c42f92c47cb8b66d3c52e61a4fa",
            "d44767eba3484138baa1dc60ace9133c",
            "e723c84d3179447187f7e04b30137740",
            "6ab13e8d3ac54ec7900526aa77ae9971",
            "7a01e82c099642f294bacd4fc68e9ddf",
            "e1980db1603742cb956b9c81ff2f5f5a",
            "ce09433b7ab44933ac0e2dd891676a82",
            "0adbe00c79234ec1839fb068e541c57d",
            "95ed9f3f5bff4a819f1816fc760f8a3b",
            "9742554df11e4a2898d77dc54f471a8a",
            "fe98ce98fd74430d8e39e9292b1d3b8b",
            "505e4ec797a24b40b69714a3c1fd9217",
            "88b1a9815153426ca2c816c891b7056a",
            "f3ed19b5380a419bbdb595bbea977cf4",
            "ab47b9ef2d074bc9a49a892fff212fc9",
            "28096235e1744e67bdd29d007086d93c",
            "4d322f0f7a2440ef8849c83b79572c7b",
            "a20424c90c67478d9f057d818f8144d2",
            "bb1178d2e15f45bd99a8ae43544cc735",
            "6b1fb5f441384427bdbb180ed4afa063",
            "7ebaa5eb28c74ccba83ebd8e86640a78",
            "0d285a83af724ec69dd96f200da48bc3",
            "acb8a4c15ffe490e83a69ffd5a79a89d",
            "ab9d660d64aa43d5b1557cb6555af7be",
            "36b66db9a42f41b090a54c460a898eb3",
            "24030269bcab4959bc31343593ce4910",
            "c8beee6e43804d2481e9bcccecb8652b",
            "9fb58ab4297c46749d0394da6a1bcff8",
            "145868494471428980e88347773d0a69",
            "2af5591087b34fa39fb841bee0c43280",
            "ed5c215f958243a4b732f8aaab43b393",
            "6faa265242674c80b0f2a85cf925bdc8",
            "1da179f4325f4d5e9b00b63c126fd1d6",
            "8529940a510046aa80accb4fe4356714",
            "1f8c06bb1a0a4fa18cbd07548d2d5363",
            "5fdb742315cb4881980bfea3a23a4ed7",
            "e91f30cf13d84875bdd88e119b2f021a",
            "8d6564ea7e24452e953541a6a1f0d7de",
            "ee0adbb169ca43a1893dc7855964cd5a",
            "08e7830b0dd4446086e2b83cb88dc9ed",
            "807bc969ea574ed8a3ec3dbafe202a93",
            "b92bc6fb22a04c52a252ea76151a04ee",
            "c05e4c44269e4daaa4f70625c4ceb92d",
            "45800cb5a10143b294f98953286b149b",
            "67273635f7974d9890091d9e80ff246f",
            "f56685e98af6455c88fccf5b84c2dcbc",
            "836e22cd747c4e63a6773e0f6630f5e5",
            "d535cfac3b0e4afd906b93a6d3689bab",
            "9ae18b6fa6f4483ca39e044588aee4ed",
            "bb74fc3873fe4006a3c74ed935c245cf",
            "768bb5a077c14a12beaaf6916f424099",
            "2a7152ee19174c40b6f55ba3af79bd9b",
            "9bceaf6a995347db86ca7e4712e73d06",
            "5f27c9e1621d411e963249abf40b3e7a",
            "dc7a4c8c2cab4b1b992a905c7efd017e",
            "f28e560fe65c464b82e71fca80883127",
            "e2fb823f66bb4d5898f62fb88be532a7",
            "316421b825b345d1b665f1ef29911674",
            "9a2829fbaca3406ba262bb0af86883ff",
            "5d957da072ab47aabccd3bde179c8f95",
            "0918db58bb6e4899b2ed193351b0882a",
            "b042aec45eaa4912a06e23729fd7bbba",
            "b88a60ad899940128a1bc1279fdc4904",
            "5472544006f84d7a80472541672f9185",
            "88dc8825fc4740e888161a1926f5a2cc",
            "03ee4f4a484045e0ab92d107c86c9906",
            "2e3aa40ad0304173be72a47b93a9f281",
            "9aabc0b59f7f4f26a33b8f30a74123f1",
            "1c48f009b6ca4ca4a03bcb8cca10fc02",
            "2afdb4d19fc1473ca133976b136d08f6",
            "ac0c79fe3ee545a0a18052be668e4a5a",
            "aff7dba8eb354d0bbbb9690decfd3ba0",
            "2f4073e0ebd74bafbb8ca68e88d66e3b",
            "f62c2e1ac5d9412b82191b8ae64eaafd",
            "25869888819b404ba6ded05baa9ed317",
            "bfb036d3fd84485399e00f34d08f4200",
            "ed7ab7c5e7094e18af98bced2b23890d",
            "a6e6ba18a7b84b17b7cd1825f71fe951",
            "f17f1597195249118cf5e8d22978743c",
            "9f7882871c59493fbbb411c40f896564",
            "a8fa5972ffc24cebb8cafc131f1e130d",
            "f5055039e35d4cd4b59daca04db36a74",
            "7452d994bb194fe5a5fb66ab814fc61f",
            "acc2a4dbcb354b7d8b08dcd4734ca0dc",
            "d9bf4115a5bd47a18d9a96b36e6b8056",
            "00375656620d404aa4d25cbbb1f4b096",
            "a5ba03d1d5fb47f5a2dce4165aaa897d",
            "0f56b881702146dd9534d3085a33fca2",
            "25fce1d783cc4e278e5d001e5571928a",
            "c5ebb58e348d47cc9752bebfe5a97c45",
            "39c9006a3c8b4fb69bc7f7d55d044e4e",
            "5483a43295b44889808acf23af41afc7",
            "cbc7f4d1e15a45fe80502ef5463ec17b",
            "8bd501c695f540828c2262cd95ebe20d",
            "645e2879eff946e0ada03c0164ed15e1",
            "0895c3510a1d40f495351d6e62d299d9",
            "1b3397190ef64c99b6208c1df6240f79",
            "f9f6375126784c2787fbc10b86c04d5c",
            "11f3400ea8c040e3b84c6a63320e72f8",
            "8194c52c42254eae8dbe2a20a0004793",
            "1c99eb2e47dc4532ac036da05192e46a",
            "e17a737e4eac4734b8eb8e26e58ddcc2",
            "8eb70c957ab94c1a9bfc590deb163894",
            "f8e5d185c7df4cacb5fc005e9267a1bd",
            "626570e6546f43229948a4a149d710d6",
            "e8d0828c88554e67baf092be51442b99",
            "fd48b66c81bb4310810bce61d4a6d0b5",
            "5f24890620254e2aacf176e0455efde0",
            "189ac65c54964b0abfc010a7e309f069",
            "2650937012074aed8261c9fa2a989d79",
            "086181e86d6a4c24a0d29b0d1d56c197"
          ]
        },
        "id": "OJXpOgBFuSrc",
        "outputId": "4119b4d5-0edf-4293-b428-8da31018b534"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0c03444db784eefa3a77e9bf8c106c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dc2d9860e3e4a75a26145c57f57c840"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eef856b26ace4e9b9ae002dab25821dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bde925d7fe94489acf0a892dd4ea6b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b412c6caea304d1d85eec27607a0603e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6a40906ee274c26925886bbd1571b74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a46fb8506c834cd88e0e1a8285a5a37e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d70d4c81c2db4e5897868a39127daa09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d44767eba3484138baa1dc60ace9133c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88b1a9815153426ca2c816c891b7056a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab9d660d64aa43d5b1557cb6555af7be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f8c06bb1a0a4fa18cbd07548d2d5363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f56685e98af6455c88fccf5b84c2dcbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2fb823f66bb4d5898f62fb88be532a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9aabc0b59f7f4f26a33b8f30a74123f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f17f1597195249118cf5e8d22978743c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5ebb58e348d47cc9752bebfe5a97c45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c99eb2e47dc4532ac036da05192e46a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 25:28, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.408800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.661100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.214700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.444700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.177700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.366500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.173800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.467000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>1.157700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.542600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "# QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "\n",
        "# base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frlSLPin4IJ4"
      },
      "outputs": [],
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7e337f-3be4-4292-c570-66e152f50b49",
        "id": "6s34XFE-A8iO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate human-like language outputs. It is typically trained on a large dataset of text, such as books, articles, or websites, and is designed to generate text that is similar to the training data.\n",
            "\n",
            "Large language models are often used for natural language processing tasks such as text classification, sentiment analysis, and machine translation. They are also used for generating text, such as chatbots, and for generating creative content, such as poetry or stories.\n",
            "\n",
            "Some examples of large language models include:\n",
            "\n",
            "* BERT (Bidirectional Encoder Representations from Transformers): A popular large language model developed by Google that is trained on a large dataset of text and is designed to generate human-like language outputs.\n",
            "* LLaMA (LLaMA:\n"
          ]
        }
      ],
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2St7nRNAAGFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
